{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNz6HWRBPqhxskw+GTWAX0e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beinghorizontal/llama_trader/blob/main/llama_trader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline,\n",
        ")\n",
        "from peft import LoraConfig\n",
        "# from trl import SFTTrainer\n"
      ],
      "metadata": {
        "id": "Zt3kH-0qJgju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from Huggingface\n",
        "def load_stock_trading_qa_dataset():\n",
        "    dataset = load_dataset(\"yymYYM/stock_trading_QA\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Format the dataset for fine-tuning\n",
        "def format_dataset(dataset):\n",
        "    formatted_dataset = dataset.map(\n",
        "        lambda example: {\n",
        "            \"text\": f\"<|system|>\\nYou are a helpful AI assistant that provides accurate information about stock trading and market analysis.\\n<|user|>\\n{example['question']}\\n<|assistant|>\\n{example['answer']}\"\n",
        "        }\n",
        "    )\n",
        "    return formatted_dataset\n"
      ],
      "metadata": {
        "id": "Tn-R1JOFJpMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Ollama modelfile\n",
        "def create_ollama_modelfile(model_path, model_name=\"llama3-stock-qa\"):\n",
        "    modelfile_content = f\"\"\"\n",
        "FROM {model_name}:latest\n",
        "\n",
        "PARAMETER temperature 0.7\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER top_k 50\n",
        "\n",
        "SYSTEM You are a helpful AI assistant specialized in stock trading, financial markets, and quantitative analysis.\n",
        "\"\"\"\n",
        "\n",
        "    with open(f\"{model_path}/Modelfile\", \"w\") as f:\n",
        "        f.write(modelfile_content)\n",
        "\n",
        "    print(f\"Created Modelfile at {model_path}/Modelfile\")\n",
        "\n"
      ],
      "metadata": {
        "id": "S2m-fSbZJsp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbYCyXcnI0Tp"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Main function to run the fine-tuning process\n",
        "def finetune_llama3_for_stock_qa():\n",
        "    # Configuration\n",
        "    model_name = \"meta-llama/Meta-Llama-3.2-8B\"\n",
        "    output_dir = \"./llama3-stock-qa-output\"\n",
        "\n",
        "    # Load and prepare dataset\n",
        "    dataset = load_stock_trading_qa_dataset()\n",
        "    print(f\"Loaded dataset with {len(dataset['train'])} training examples\")\n",
        "\n",
        "    # Format dataset for fine-tuning\n",
        "    formatted_dataset = format_dataset(dataset[\"train\"])\n",
        "\n",
        "    # Configure BitsAndBytes for quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Load base model and tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # Configure LoRA\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\",\n",
        "            \"gate_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./checkpoints\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=2,\n",
        "        gradient_checkpointing=True,\n",
        "        optim=\"paged_adamw_32bit\",\n",
        "        learning_rate=2e-4,\n",
        "        weight_decay=0.001,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        save_steps=10,\n",
        "        save_total_limit=2,\n",
        "        group_by_length=True,\n",
        "        push_to_hub=True,\n",
        "        hub_strategy=\"checkpoint\",\n",
        "        hub_model_id=\"crossdelenna/llama_trader\",\n",
        "        hub_token=\"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\",\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=formatted_dataset,\n",
        "        peft_config=lora_config,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=2048,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_args,\n",
        "    )\n",
        "\n",
        "    def resume_from_checkpoint():\n",
        "        checkpoint = None\n",
        "        if os.path.exists(\"./checkpoints\"):\n",
        "            checkpoint_dirs = [\n",
        "                d for d in os.listdir(\"./checkpoints\") if d.startswith(\"checkpoint-\")\n",
        "            ]\n",
        "            if checkpoint_dirs:\n",
        "                # Get the latest checkpoint\n",
        "                latest_checkpoint = max(\n",
        "                    checkpoint_dirs, key=lambda x: int(x.split(\"-\")[1])\n",
        "                )\n",
        "                checkpoint = os.path.join(\"./checkpoints\", latest_checkpoint)\n",
        "                print(f\"Resuming from checkpoint: {checkpoint}\")\n",
        "        return checkpoint\n",
        "\n",
        "    # Train with resume functionality\n",
        "    checkpoint = resume_from_checkpoint()\n",
        "    trainer.train(resume_from_checkpoint=checkpoint)\n",
        "    # Start training\n",
        "    print(\"Starting fine-tuning...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model\n",
        "    trainer.save_model(output_dir)\n",
        "    print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "    # Create Ollama modelfile\n",
        "    create_ollama_modelfile(output_dir)\n",
        "\n",
        "    # Push the model to your repository\n",
        "    trainer.push_to_hub(repo_id=\"crossdelenna/llama_trader\", private=False)\n",
        "    print(\"Model successfully pushed to Hugging Face Hub!\")\n",
        "\n",
        "    # Test the model\n",
        "    print(\"Testing the model...\")\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=512,\n",
        "    )\n",
        "\n",
        "    test_question = \"What is the ARIMA model and how does it help in predicting stock market trends?\"\n",
        "    prompt = f\"<|system|>\\nYou are a helpful AI assistant that provides accurate information about stock trading and market analysis.\\n<|user|>\\n{test_question}\\n<|assistant|>\\n\"\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    print(result[0][\"generated_text\"])\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    finetune_llama3_for_stock_qa()\n"
      ]
    }
  ]
}